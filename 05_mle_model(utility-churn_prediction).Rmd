---
title: "05_ MLE model for utility"
author: "Inwon" 
date: '2022 3 23 '
output: html_document
---
#GOAL : 유저의 효용(unobservable variable)을 각 변수별 Maximum Likelihood를 통해 구하기 
#GOAL : 효용을 바탕으로 period 1의 훈련모델로 period2의 이탈가능성 예측하기 

#1. 데이터 소환 
```{r}
datatcut<- read.csv("36_dataoriginaltype2.csv")
datatcut <- datatcut[,-1]
library(dplyr)
```

2) usertype by kmeans 345 - 가장 robust 한 데이터셋 
```{r}
# 극초반 제외 3~5주차에서 pvp 활동의 비중을 보고 나눈 데이터 군(competence 중시) 
usertype2 <- read.csv("36_kmeansbyt345") 
#meanLevup :주간 레벨업 평균 #meanRE: 외적 보상과 성취를 주는 주는 pvp, 던전의 선택 비중  
usertype2 <- usertype2[,c(2,5)] 
```
```{r}
dat2 <- inner_join(datatcut, usertype2, by="id")
```
```{r}
dat1 <- dat2[,-c(17:20)]
```


3) 데이터 스케일링 

```{r}
#dat1 : Arena, Pvp, Dungeon, Quest, Total : 각 활동별 플레이 실제 시간(min)/ 주 
summary(dat1)
```

```{r}
dat1.scaled <- dat1[,c(1,2,4)]
dat1.scaled$user <- dat1$user # 유저타입 : achiever = 0, experiencer =1, determined by t=3~5의 활동성향 

dat1.scaled$Arena <- scale(dat1$Arena) #주당 Arena 참여시간 
dat1.scaled$Pvp <- scale(dat1$Pvp)    # 주당 Pvp 참여시간 (Arena, Battleground )
dat1.scaled$Dungeon <- scale(dat1$Dungeon) # 주당 던전참여시간 
dat1.scaled$Quest <- scale(dat1$Quest) #주당 던전, 바다 등 퀘스트 참여시간 

dat1.scaled$Asize <- scale(dat1$Asize) #해당 주의 가입한 길드의 가입자 수 
dat1.scaled$Bcumnperiod <- scale(dat1$Bcumnperiod) #해당 주까지 평균적으로 길드에 체류한 주 수 
dat1.scaled$Cchange <- dat1$Cchange #해당 주까지 누적 길드 변경 회수 
dat1.scaled$Dsolo <- dat1$Dsolo #해당 주까지 길드에 가입하지 않고 플레이한 경험 누적 주수 
dat1.scaled$Eweeklyplay <- scale(dat1$Eweeklyplay) # 해당 주 게임 플레이시간 / 전체 시간 
dat1.scaled$Jcumt <- dat1$Jcumt #해당 주까지 누적 미접속 주 수 
```





#2. mle 함수 만들기

# Utility = XB + e , (X = vector of x_i, i= 1,,,n ) (e for error term ) 
# Getting Beta with maximum likelihood estimation with logit model (y = 이탈, 3주 이상 미접속 시 이탈 간주)

#1)Maximum likelihood for utilty 
```{r}
#Utility = XB + e 이며 

library(gtools)

mle1 <- function(formular,  data){
  data <- data
  xinput <- formular(data)
  
  ll <- function(theta, x, y){
    y <- data[,3] #leave 활용 
    x <- xinput
    c <- data[,4] #유저타입은 constant 처리 
    beta <- theta[1:ncol(x)]
    
    loglik <- sum(-y*log(1+exp(-(x%*%beta+c))) - (1-y)*log(1+exp(x%*%beta+c)))
    return(-loglik)
  }
    
  n= dim(xinput)[2]
  theta.start = rep(0, (dim(xinput)[2]))
  mle = optim(theta.start, ll, x=xinput, y=y, hessian =T, method="BFGS") 
  
  var = colnames(xinput)
  beta = mle$par
  vcov = solve(mle$hessian)
  se = sqrt(diag(vcov))
  tvalue = beta/se
  pvalue = pt(tvalue, df= n-1 )
  p = scales::pvalue(pvalue)
  star= stars.pval(pvalue)

  print(cbind(var,beta,p,star, se,tvalue, pvalue))
}


```


2) Xinput 만들기 
```{r}
# multicollinearity 고려, 상관관계가 적은 변수들 선발(under 0.2)
#mle0 : Arena(4), Asize(8), Dsolo(11), Jcumt(13)
Input0 <- function(data){
    xinput = data[,c(5,9,12,14)]
    xinput <- as.matrix(xinput)
    nobs <- dim(xinput)[1]
    d <- dim(xinput)[2]
    bdummy = rep(1,nobs)%x% diag(1,d)
    intercept = bdummy[,1]
    xinput <- cbind(intercept,xinput)
  return(xinput)
}
```



3. Data split and mle prediction 

1)데이터 나누기 

```{r}
#data split by t : p1 의 데이터로 p2 예측하기 
# period 1: t = 6 : 9 # period 2: t = 10: 13

# max(t) <13 인 데이터 날리기 
dat1s <- dat1.scaled%>%
  group_by(id)%>%
  mutate(maxt = max(t))
dat1s <- dat1s%>%
  filter(maxt>=13)
```

```{r}
#data split by id 
id <- unique(dat1s$id)
set.seed(100)
sampleid = sort(sample(id, 880, replace = F))

testdata = dat1.scaled[dat1.scaled$id %in% sampleid, ]
traindata = dat1.scaled[!(dat1.scaled$id %in% sampleid),]
```

```{r}
#data split by id & period 
#Train , period 1
trdata <- traindata%>%
  filter(t %in% c(6,7,8,9))
#Train, period 2
trdata2 <- traindata%>%
  filter(t %in% c(10,11,12,31))
#Test, period 1
tedata <- testdata%>%
  filter(t %in% c(6,7,8,9))
#Test, period 2
tedata2 <- testdata%>%
  filter(t %in% c(10,11,12,13))
```

2) model training 
```{r}
# getting beta 
mle01 <- mle1(Input0, trdata)
```
```{r}
# fuction for combining beta and x  
model01 <- function(summary, data){
  b0 = as.numeric(summary[1,2])
  b1 = as.numeric(summary[2,2])
  b2 = as.numeric(summary[3,2])
  b3 = as.numeric(summary[4,2])
  b4 = as.numeric(summary[5,2])

  U <- b0+ b1*data$Arena + b2*data$Asize+ b3*data$Dsolo + b4*data$Jcumt
  return(U)
}
```

```{r}
U.trp1 <- model01(mle01, trdata)
```
```{r}
hist(U.trp1, breaks=50, col='white')
```

```{r}
#Gumbel distribution parameter 구하기 
library(ExtDist)
par <- eGumbel(U.trp1, method = "moments")
par
```
```{r}
loc= -5.441298	
scale = 4.019328		

n=length(U.trp1)
list = list()
for(i in 1:n) {
  list[i] = dgumbel::pgumbel(U.trp1[i], location = loc, scale= scale	)
}
ptrp1<- as.numeric(list)
```
```{r}
P.trp1<- ifelse(ptrp1>=0.5,1,0) 
```
```{r}
library(ROCR)
pred.trp1 <- prediction(P.trp1, trdata$leave)
plot(performance(pred.trp1,"tpr","fpr"), main = "trained model(trp1)")
```

```{r}
library(caret)
trp1.leave <- as.factor(trdata$leave)
trp1.pred <- as.factor(P.trp1)

confusionMatrix(trp1.pred, trp1.leave, mode='everything') #0.7938  # F1: 0.8103 
```

3)model testing - training dataset의 p2 예측하기 

```{r}
U.trp2 <- model01(mle01, trdata2)
```

```{r}
hist(U.trp2, breaks=50, col='white')
```

```{r}
#파라미터 구하기 
par2 <- eGumbel(U.trp2, method = "moments")
par2
```
```{r}
loc = -5.472210	
scale = 5.856611	


n=length(U.trp2)
list = list()
for(i in 1:n) {
  list[i] = dgumbel::pgumbel(U.trp2[i], location = loc, scale= scale	)
}
ptrp2<- as.numeric(list)
```
```{r}
P.trp2<- ifelse(ptrp2>=0.5,1,0) 
```
```{r}
library(ROCR)
pred.trp2 <- prediction(P.trp2, trdata2$leave)
plot(performance(pred.trp2,"tpr","fpr"), main = "Prediction on train data period2")
```
```{r}
trp2.leave <- as.factor(trdata2$leave)
trp2.pred <- as.factor(P.trp2)

confusionMatrix(trp2.pred, trp2.leave, mode='everything') #0.8777 #F1 :  0.8682
```


3)model testing - test dataset의 p2 예측하기 

```{r}
U.tep2 <- model01(mle01, tedata2)
```

```{r}
hist(U.tep2, breaks=50, col='white')
```


```{r}
#파라미터 구하기 
par3 <- eGumbel(U.tep2, method = "moments")
par3
```
```{r}
loc = -7.556339	
scale = 4.950907	


n=length(U.tep2)
list = list()
for(i in 1:n) {
  list[i] = dgumbel::pgumbel(U.tep2[i], location = loc, scale= scale	)
}
ptep2<- as.numeric(list)
```
```{r}
P.tep2<- ifelse(ptep2>=0.5,1,0) 
```
```{r}
library(ROCR)
pred.tep2 <- prediction(P.tep2, tedata2$leave)
plot(performance(pred.tep2,"tpr","fpr"), main = "Prediction on train data period2")
```

```{r}
tep2.leave <- as.factor(tedata2$leave)
tep2.pred <- as.factor(P.tep2)

confusionMatrix(tep2.pred, tep2.leave, mode='everything') #Accuracy: 0.7112 # F1:0.7361
```

#Conclusion 

# - 모델의 한 달 후 이탈가능성 예측도는 0.7112 

